{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f20ef83b-888c-46be-b256-551fdb3fe834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Install statements.\"\"\"\n",
    "\n",
    "!pip install -q -U langchain-community\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q pypdf\n",
    "!pip install -q openai\n",
    "!pip install -q chromadb\n",
    "!pip install -q tiktoken\n",
    "!pip install -q langchain faiss-cpu transformers\n",
    "!pip install -q langchain-chroma\n",
    "!pip install -q langchain-ollama\n",
    "!pip install -q huggingface_hub\n",
    "!pip install -q ipywidgets\n",
    "!pip install -q langchain-huggingface\n",
    "!pip install -q einops\n",
    "!pip install -q transformers\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf82637d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jimmy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Import statements.\"\"\"\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "from langchain.document_loaders import TextLoader, PyPDFDirectoryLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline, ChatHuggingFace\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from time import time, gmtime, strftime\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d97e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_chunk(pdf_loc=None, csv_loc=None):\n",
    "    \"\"\"Load and chunk documents from specified locations, for use with ChromaDB RAG system.\n",
    "\n",
    "    Args:\n",
    "      pdf_loc: path to directory containing pdf file(s)\n",
    "      csv_loc: path to directory containing csv file(s)\n",
    "    \n",
    "    Returns:\n",
    "      list of documents to use with Chroma\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "    # Load pdf documents\n",
    "    pdf_chunks = None\n",
    "    if pdf_loc:\n",
    "      pdf_loader = PyPDFDirectoryLoader(pdf_loc)\n",
    "      pdf_pages = pdf_loader.load()\n",
    "      pdf_chunks = text_splitter.split_documents(pdf_pages)\n",
    "\n",
    "    # Load csv documents\n",
    "    csv_chunks = None\n",
    "    if csv_loc:\n",
    "      csv_loader = DirectoryLoader(csv_loc)\n",
    "      csv_pages = csv_loader.load()\n",
    "      csv_chunks = text_splitter.split_documents(csv_pages)\n",
    "\n",
    "    output = []\n",
    "    if pdf_chunks:\n",
    "      output.append(pdf_chunks)\n",
    "    if csv_chunks:\n",
    "      output.append(csv_chunks)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f677f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query, database):\n",
    "    \"\"\"Given a query, create a prompt and receive a response.\n",
    "\n",
    "    Args:\n",
    "      query: The query to answer.\n",
    "      database: The colleciton of documents to use for RAG (assumes ChromaDB)\n",
    "    \n",
    "    Returns:\n",
    "      response received from the LLM model used\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up context\n",
    "    docs_chroma = database.similarity_search_with_score(query, k=5)\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc, _score in docs_chroma])\n",
    "\n",
    "    # Set up prompt\n",
    "    prompt_template = \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "    {context}\n",
    "    Answer the question based on the above context: {question}.\n",
    "    Add a new line after every sentence.\n",
    "    Do not mention any information which is not contained within the context.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Load context and query into prompt\n",
    "    prompt_template = ChatPromptTemplate.from_template(prompt_template)\n",
    "    prompt = prompt_template.format(context=context_text, question=query)\n",
    "\n",
    "    # Get answer from LLM\n",
    "    if (model_choice == \"openai\"):\n",
    "        response = model.predict(query)\n",
    "    else:\n",
    "        if (model_choice in local_models):\n",
    "            response = model.invoke(prompt)\n",
    "        else:\n",
    "            response = model.invoke(prompt)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11fdc414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_chroma_load(modified_times_loc, chroma_loc, cur_embedding, other_locs=list()):\n",
    "    \"\"\"Determine if Chroma should load from directory or start a new run.\n",
    "\n",
    "    Args:\n",
    "      modified_times_loc: Location of files saving last modified times for each embedding model.\n",
    "      chroma_loc: Location of persistent directory for Chroma.\n",
    "      cur_embedding: Current choice of embedding model, to check if a directory for said model exists.\n",
    "      other_locs: List of locations with context docs to be checked for changes. Defaults to empty list.\n",
    "    \n",
    "    Returns:\n",
    "      Boolean representing if Chroma should use saved files or create new files.\n",
    "    \"\"\"\n",
    "    # Get last modified times for each of the directories holding context information\n",
    "    context_times = [os.path.getmtime(folder) for folder in other_locs]\n",
    "\n",
    "    # Get last modified time for specific embedding model, making file and setting time to 0 if none exists\n",
    "    cur_embedding_chroma_mod = None\n",
    "    cur_embedding_chroma_mod_loc = f\"{modified_times_loc}{cur_embedding}.txt\"\n",
    "    if not os.path.exists(cur_embedding_chroma_mod_loc):\n",
    "        # Make file with \"time\" of 0 if no file exists for this embedding model\n",
    "        with open(cur_embedding_chroma_mod_loc, \"w\") as outf:\n",
    "            outf.write(\"0\")\n",
    "    with open(cur_embedding_chroma_mod_loc, \"r\") as inf:\n",
    "        for line in inf:\n",
    "            cur_embedding_chroma_mod = float(line.strip())\n",
    "            break\n",
    "\n",
    "    # Get booleans for determining if Chroma should load\n",
    "    chroma_dir_exists = os.path.exists(f\"{chroma_loc}{cur_embedding}\\\\\")\n",
    "    context_modified = any([mod > cur_embedding_chroma_mod for mod in context_times])\n",
    "\n",
    "    # Determine if Chroma should load\n",
    "    if context_modified or not chroma_dir_exists:\n",
    "        should_load = False\n",
    "    else:\n",
    "        should_load = True\n",
    "    \n",
    "    return should_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610cca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Desktop\\SchoolStuff\\CSE_5095_Spring2025\\Project\\Code\\context_files\\pdf_files\\\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'D:\\\\Desktop\\\\SchoolStuff\\\\CSE_5095_Spring2025\\\\Project\\\\Codecontext_files\\\\pdf_files\\\\'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Flag to determine if Chroma should load from persistent directory\u001b[39;00m\n\u001b[0;32m     31\u001b[0m context_locs \u001b[38;5;241m=\u001b[39m [PDF_ROOT, CSV_ROOT]\n\u001b[1;32m---> 32\u001b[0m chroma_load \u001b[38;5;241m=\u001b[39m \u001b[43mset_chroma_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODIFIED_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCHROMA_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_choice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_locs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Flag to determine if program is running locally or not\u001b[39;00m\n\u001b[0;32m     35\u001b[0m local \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m, in \u001b[0;36mset_chroma_load\u001b[1;34m(modified_times_loc, chroma_loc, cur_embedding, other_locs)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Determine if Chroma should load from directory or start a new run.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m  Boolean representing if Chroma should use saved files or create new files.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Get last modified times for each of the directories holding context information\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m context_times \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetmtime(folder) \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m other_locs]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Get last modified time for specific embedding model, making file and setting time to 0 if none exists\u001b[39;00m\n\u001b[0;32m     17\u001b[0m cur_embedding_chroma_mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Determine if Chroma should load from directory or start a new run.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m  Boolean representing if Chroma should use saved files or create new files.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Get last modified times for each of the directories holding context information\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m context_times \u001b[38;5;241m=\u001b[39m [\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetmtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m other_locs]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Get last modified time for specific embedding model, making file and setting time to 0 if none exists\u001b[39;00m\n\u001b[0;32m     17\u001b[0m cur_embedding_chroma_mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\genericpath.py:55\u001b[0m, in \u001b[0;36mgetmtime\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetmtime\u001b[39m(filename):\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the last modification time of a file, reported by os.stat().\"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mst_mtime\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'D:\\\\Desktop\\\\SchoolStuff\\\\CSE_5095_Spring2025\\\\Project\\\\Codecontext_files\\\\pdf_files\\\\'"
     ]
    }
   ],
   "source": [
    "\"\"\"Define variables.\"\"\"\n",
    "\n",
    "# File system navigation\n",
    "EMBEDDING_ROOT = \"D:\\\\Desktop\\\\AI\\\\Embeddings\\\\\"\n",
    "MODEL_ROOT = \"D:\\\\Desktop\\\\AI\\\\LLMs\\\\\"\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "PDF_ROOT = os.path.join(PROJECT_ROOT, \"context_files\\\\pdf_files\\\\\")\n",
    "CSV_ROOT = os.path.join(PROJECT_ROOT, \"context_files\\\\csv_files\\\\\")\n",
    "CHROMA_ROOT = os.path.join(PROJECT_ROOT, \"chroma_db_files\\\\\")\n",
    "MODIFIED_ROOT = os.path.join(CHROMA_ROOT, \"(0)modified-times\\\\\")\n",
    "OUTPUT_ROOT = os.path.join(PROJECT_ROOT, \"output_files\\\\\")\n",
    "\n",
    "# Embedding to use, determines if running online\n",
    "ollama_embeddings = [\"nomic-embed-text\", \"mxbai-embed-large\"]\n",
    "local_embeddings = [\"nomic-embed-text-v1.5\", \"bert-base-uncased\"]\n",
    "online_embeddings = [\"openai\"]\n",
    "embeddings_choice = ollama_embeddings[1]\n",
    "\n",
    "# Model to use, determines if running online\n",
    "ollama_models = [\"deepseek-r1:7b\", \"deepseek-r1:32b\", \"deepseek-r1:70b\", \"llama3.3\", \"mistral\", \"deepseek-r1:671b\"]  # Don't use 671b\n",
    "local_models = [\"bert-base-uncased\", \"gpt2\", \"Mistral-7B-Instruct-v0.3\", \"zephyr-7b-beta\", \"DarkForest-20B-v3.0\"]\n",
    "online_models = [\"openai\"]\n",
    "model_choice = ollama_models[1]\n",
    "\n",
    "# Flags\n",
    "save_output = True     # Flag to determine if the response should be saved to a text file\n",
    "\n",
    "# Flag to determine if Chroma should load from persistent directory\n",
    "context_locs = [PDF_ROOT, CSV_ROOT]\n",
    "chroma_load = set_chroma_load(MODIFIED_ROOT, CHROMA_ROOT, embeddings_choice, other_locs=context_locs)\n",
    "\n",
    "# Flag to determine if program is running locally or not\n",
    "local = True\n",
    "if ((embeddings_choice == \"openai\") or (model_choice == \"openai\")):\n",
    "    local = False\n",
    "\n",
    "# For api keys\n",
    "%env OPENAI_API_KEY = \"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\"\n",
    "OPENAI_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74751f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "C:\\Users\\jimmy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\unstructured\\partition\\csv.py:58: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  dataframe = pd.read_csv(file, header=ctx.header, sep=ctx.delimiter, encoding=encoding)\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "C:\\Users\\jimmy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\unstructured\\partition\\csv.py:58: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  dataframe = pd.read_csv(file, header=ctx.header, sep=ctx.delimiter, encoding=encoding)\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generate embeddings and manage vectors (assuming only using PDFs).\"\"\"\n",
    "\n",
    "# Set up ChromaDB path and embedding based on embeddings_choice\n",
    "if not os.path.exists(f\"{CHROMA_ROOT}{embeddings_choice}\"):\n",
    "    try:\n",
    "        os.mkdir(f\"{CHROMA_ROOT}{embeddings_choice}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error:\\n{e}\")\n",
    "        sys.exit(1)\n",
    "chroma_path = f\"{CHROMA_ROOT}{embeddings_choice}\\\\\"\n",
    "if local:\n",
    "    if (embeddings_choice in ollama_embeddings):\n",
    "        embeddings = OllamaEmbeddings(model=embeddings_choice)\n",
    "    elif (embeddings_choice in local_embeddings):\n",
    "        model_kwargs = {'trust_remote_code': True}\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=f\"{EMBEDDING_ROOT}{embeddings_choice}\\\\\", model_kwargs=model_kwargs)\n",
    "else:\n",
    "    if (embeddings_choice == \"openai\"):\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_KEY)\n",
    "\n",
    "# Set up ChromaDB based on whether or not pre-saved information should be used\n",
    "if chroma_load:\n",
    "    db_chroma = Chroma(embedding_function=embeddings, persist_directory=chroma_path)\n",
    "else:\n",
    "    # Give context information to Chroma\n",
    "    # Not sure best way to handle, so create Chroma with first set of documents, then add any other documents\n",
    "    chunks = load_and_chunk(pdf_loc=PDF_ROOT, csv_loc=CSV_ROOT)\n",
    "    db_chroma = Chroma.from_documents(chunks[0], embeddings, persist_directory=chroma_path)\n",
    "    for i in range(1, len(chunks)):\n",
    "        db_chroma.add_documents(documents=chunks[i])\n",
    "\n",
    "    # Save current time as last modified time for context information for this embedding\n",
    "    with open(f\"{MODIFIED_ROOT}{embeddings_choice}.txt\", \"w\") as outf:\n",
    "        outf.write(f\"{time()}\")\n",
    "\n",
    "    # Flip chroma_load to True, to allow rerunning this section without remaking Chroma database\n",
    "    chroma_load = True\n",
    "\n",
    "# Set up model based on model_choice\n",
    "if local:\n",
    "    if (model_choice in ollama_models):\n",
    "        model = ChatOllama(model=model_choice)\n",
    "    elif (model_choice in local_models):\n",
    "        #pipe = pipeline(model=f\"{MODEL_ROOT}{model_choice}\\\\\", task=\"text-generation\", max_length=1000)\n",
    "        #llm = HuggingFacePipeline(pipeline=pipe)\n",
    "        llm = HuggingFacePipeline.from_model_id(model_id=f\"{MODEL_ROOT}{model_choice}\\\\\", task=\"text-generation\", device=0)\n",
    "        model = ChatHuggingFace(llm=llm)\n",
    "else:\n",
    "    if (model_choice == \"openai\"):\n",
    "        model = ChatOpenAI(openai_api_key=OPENAI_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "326591d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Does NR4A1 cause an increase in BAX in chondrocytes with osteoarthritis?\n",
      "\n",
      "According to Ansari et al., NR4A1 is likely to facilitate OA chondrocyte apoptosis, which is associated with p38 MAPK and mitochondrial apoptosis pathway.\n",
      "\n",
      "The relationship between NR4A1 and BAX in chondrocytes with osteoarthritis is not directly stated in the context provided by Shi et al. or Ansari et al.\n",
      "\n",
      "However, it is mentioned that lysosomal dysfunction induces apoptosis in chondrocytes through BAX-mediated mitochondrial damage and release of cytochrome c, as per Alvarez-Garcia et al. and also supported by Ansari et al.\n",
      "\n",
      "There is no direct information provided in the context about NR4A1 causing an increase in BAX in chondrocytes with osteoarthritis.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Receive answer to a query, with ability to save to .txt file.\"\"\"\n",
    "\n",
    "# Receive response to query\n",
    "query = \"Does NR4A1 cause an increase in BAX in chondrocytes with osteoarthritis?\"\n",
    "response = answer_query(query, db_chroma)\n",
    "\n",
    "# Create output for question and response\n",
    "output = \"\"\n",
    "output += f\"Query: {query}\\n\\n\"\n",
    "\n",
    "# Extract string of response, if needed\n",
    "if not isinstance(response, str):\n",
    "    response = response.content\n",
    "\n",
    "# Add response to output\n",
    "if local:\n",
    "    if (model_choice == \"Mistral-7B-Instruct-v0.3\"):\n",
    "        prompt_end = response.find(\"[/INST]\")\n",
    "        output += response[(prompt_end + 7):]\n",
    "    elif (\"deepseek-r1\" in model_choice):\n",
    "        think_end = response.find(\"</think>\")\n",
    "        output += response[(think_end + 8):]\n",
    "    else:\n",
    "        output += response\n",
    "else:\n",
    "    output += response\n",
    "\n",
    "# Write output to file if save flag is set\n",
    "if save_output:\n",
    "    cur_time = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "    with open(f\"{OUTPUT_ROOT}RAG_Output_{cur_time}.txt\", \"w\") as outf:\n",
    "        outf.write(output)\n",
    "\n",
    "# Print response for convenience\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b168d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f20ef83b-888c-46be-b256-551fdb3fe834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Install statements.\"\"\"\n",
    "\n",
    "!pip install -q -U langchain-community\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q pypdf\n",
    "!pip install -q openai\n",
    "!pip install -q chromadb\n",
    "!pip install -q tiktoken\n",
    "!pip install -q langchain faiss-cpu transformers\n",
    "!pip install -q langchain-chroma\n",
    "!pip install -q langchain-ollama\n",
    "!pip install -q huggingface_hub\n",
    "!pip install -q ipywidgets\n",
    "!pip install -q langchain-huggingface\n",
    "!pip install -q einops\n",
    "!pip install -q transformers\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf82637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import statements.\"\"\"\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "from langchain.document_loaders import TextLoader, PyPDFDirectoryLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline, ChatHuggingFace\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from time import time, gmtime, strftime\n",
    "from sys import exit\n",
    "from shutil import rmtree\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d97e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_chunk(splitter, pdf_loc=None, csv_loc=None):\n",
    "    \"\"\"Load and chunk documents from specified locations.\n",
    "\n",
    "    Args:\n",
    "      splitter: The text splitter to use.\n",
    "      pdf_loc: Path to directory containing pdf file(s).\n",
    "      csv_loc: Path to directory containing csv file(s).\n",
    "    \n",
    "    Returns:\n",
    "      Dictionary of chunked documents\n",
    "    \"\"\"\n",
    "    # Load pdf documents\n",
    "    pdf_chunks = None\n",
    "    if pdf_loc:\n",
    "      pdf_loader = PyPDFDirectoryLoader(pdf_loc)\n",
    "      pdf_pages = pdf_loader.load()\n",
    "      pdf_chunks = splitter.split_documents(pdf_pages)\n",
    "\n",
    "    # Load csv documents\n",
    "    csv_chunks = None\n",
    "    if csv_loc:\n",
    "      csv_loader = DirectoryLoader(csv_loc)\n",
    "      csv_pages = csv_loader.load()\n",
    "      csv_chunks = splitter.split_documents(csv_pages)\n",
    "\n",
    "    output = {'pdf': None, 'csv': None, 'txt': None}\n",
    "    if pdf_chunks:\n",
    "      output['pdf'] = pdf_chunks\n",
    "    if csv_chunks:\n",
    "      output['csv'] = csv_chunks\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f677f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query, database, num_docs):\n",
    "    \"\"\"Given a query, create a prompt and receive a response.\n",
    "\n",
    "    Args:\n",
    "      query: The query to answer.\n",
    "      database: The colleciton of documents to use for RAG (assumes ChromaDB).\n",
    "      num_docs: How many documents should be given as context information.\n",
    "    \n",
    "    Returns:\n",
    "      response received from the LLM model used\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up context\n",
    "    docs_chroma = database.similarity_search_with_score(query, k=num_docs)\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc, _score in docs_chroma])\n",
    "\n",
    "    # Set up prompt\n",
    "    prompt_template = \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "    {context}\n",
    "    Answer the question based on the above context: {question}.\n",
    "    Add a new line after every sentence.\n",
    "    Do not mention any information which is not contained within the context.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Load context and query into prompt\n",
    "    prompt_template = ChatPromptTemplate.from_template(prompt_template)\n",
    "    prompt = prompt_template.format(context=context_text, question=query)\n",
    "\n",
    "    # Get answer from LLM\n",
    "    if (model_choice == \"openai\"):\n",
    "        response = model.predict(query)\n",
    "    else:\n",
    "        if (model_choice in local_models):\n",
    "            response = model.invoke(prompt)\n",
    "        else:\n",
    "            response = model.invoke(prompt)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11fdc414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_chroma_load(modified_times_loc, chroma_loc, cur_embedding, other_locs=list()):\n",
    "    \"\"\"Determine if Chroma should load from directory or start a new run.\n",
    "\n",
    "    Args:\n",
    "      modified_times_loc: Location of files saving last modified times for each embedding model.\n",
    "      chroma_loc: Location of persistent directory for Chroma.\n",
    "      cur_embedding: Current choice of embedding model, to check if a directory for said model exists.\n",
    "      other_locs: List of locations with context docs to be checked for changes. Defaults to empty list.\n",
    "    \n",
    "    Returns:\n",
    "      Boolean representing if Chroma should use saved files or create new files.\n",
    "    \"\"\"\n",
    "    # Get last modified times for each of the directories holding context information\n",
    "    context_times = [os.path.getmtime(folder) for folder in other_locs]\n",
    "\n",
    "    # Get last modified time for specific embedding model, making file and setting time to 0 if none exists\n",
    "    cur_embed_chroma_mod = None\n",
    "    cur_embed_chroma_mod_loc = os.path.join(modified_times_loc, f\"{cur_embedding}.txt\")\n",
    "    if not os.path.exists(cur_embed_chroma_mod_loc):\n",
    "        # Make file with \"time\" of 0 if no file exists for this embedding model\n",
    "        with open(cur_embed_chroma_mod_loc, \"w\") as outf:\n",
    "            outf.write(\"0\")\n",
    "    with open(cur_embed_chroma_mod_loc, \"r\") as inf:\n",
    "        for line in inf:\n",
    "            cur_embed_chroma_mod = float(line.strip())\n",
    "            break\n",
    "\n",
    "    # Get booleans for determining if Chroma should load\n",
    "    chroma_dir_exists = os.path.exists(os.path.join(chroma_loc, f\"{cur_embedding}\\\\\"))\n",
    "    context_modified = any([mod > cur_embed_chroma_mod for mod in context_times])\n",
    "\n",
    "    # Determine if Chroma should load\n",
    "    if context_modified or not chroma_dir_exists:\n",
    "        should_load = False\n",
    "    else:\n",
    "        should_load = True\n",
    "    \n",
    "    return should_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f610cca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=\"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\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Define variables.\"\"\"\n",
    "\n",
    "# File system navigation\n",
    "EMBEDDING_ROOT = \"D:\\\\Desktop\\\\AI\\\\Embeddings\\\\\"\n",
    "MODEL_ROOT = \"D:\\\\Desktop\\\\AI\\\\LLMs\\\\\"\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "CONTEXT_ROOT = os.path.join(PROJECT_ROOT, \"context_files\\\\\")\n",
    "PDF_ROOT = os.path.join(CONTEXT_ROOT, \"pdf_files\\\\\")\n",
    "CSV_ROOT = os.path.join(CONTEXT_ROOT, \"csv_files\\\\\")\n",
    "CHROMA_ROOT = os.path.join(PROJECT_ROOT, \"chroma_db_files\\\\\")\n",
    "MODIFIED_ROOT = os.path.join(CHROMA_ROOT, \"(0)modified-times\\\\\")\n",
    "OUTPUT_ROOT = os.path.join(PROJECT_ROOT, \"output_files\\\\\")\n",
    "\n",
    "# Create structural directories, if they don't exist\n",
    "context_roots = [PDF_ROOT, CSV_ROOT]\n",
    "roots = [CHROMA_ROOT, MODIFIED_ROOT, OUTPUT_ROOT, CONTEXT_ROOT] + context_roots\n",
    "for root in roots:\n",
    "    if not os.path.exists(root):\n",
    "        try:\n",
    "            os.mkdir(root)\n",
    "        except Exception as e:\n",
    "            print(f\"Error making {root}:\\n{e}\")\n",
    "            exit(1)\n",
    "\n",
    "# Determine if context directories are empty\n",
    "need_context = False\n",
    "for root in context_roots:\n",
    "    if not os.listdir(root):\n",
    "        need_context = True\n",
    "\n",
    "# Check if context documents need to be added\n",
    "if need_context:\n",
    "    print(\"\\nYou have not supplied any documents to be used as context information.\")\n",
    "    print(\"Please do so before using this system.\")\n",
    "    exit(1)\n",
    "\n",
    "# Embedding to use, determines if running online\n",
    "ollama_embeddings = [\"nomic-embed-text\", \"mxbai-embed-large\"]\n",
    "local_embeddings = [\"nomic-embed-text-v1.5\", \"bert-base-uncased\"]\n",
    "online_embeddings = [\"openai\"]\n",
    "embeddings_choice = \"mxbai-embed-large\"\n",
    "\n",
    "# Model to use, determines if running online\n",
    "# Models: deepseek-r1:[7b|14b|32b|70b], llama3.3, mistral, mixtral:8x7b\n",
    "ollama_models = [\"deepseek-r1:7b\", \"deepseek-r1:14b\", \"deepseek-r1:32b\", \"deepseek-r1:70b\", \"llama3.3\", \"mistral\", \"mixtral:8x7b\", \"deepseek-r1:671b\"]  # Don't use 671b\n",
    "local_models = [\"bert-base-uncased\", \"gpt2\", \"Mistral-7B-Instruct-v0.3\", \"zephyr-7b-beta\", \"DarkForest-20B-v3.0\"]\n",
    "online_models = [\"openai\"]\n",
    "model_choice = \"deepseek-r1:7b\"\n",
    "\n",
    "# Flag to determine if program is running locally or not\n",
    "local = True\n",
    "if ((embeddings_choice == \"openai\") or (model_choice == \"openai\")):\n",
    "    local = False\n",
    "\n",
    "# For api keys\n",
    "%env OPENAI_API_KEY = \"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\"\n",
    "OPENAI_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74751f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate embeddings and manage vectors.\"\"\"\n",
    "\n",
    "# Set up ChromaDB path and embedding based on embeddings_choice\n",
    "cur_embed_db = os.path.join(CHROMA_ROOT, f\"{embeddings_choice}\")\n",
    "if not os.path.exists(cur_embed_db):\n",
    "    try:\n",
    "        os.mkdir(cur_embed_db)\n",
    "    except Exception as e:\n",
    "        print(f\"Error:\\n{e}\")\n",
    "        exit(1)\n",
    "if local:\n",
    "    if (embeddings_choice in ollama_embeddings):\n",
    "        embeddings = OllamaEmbeddings(model=embeddings_choice)\n",
    "    elif (embeddings_choice in local_embeddings):\n",
    "        model_kwargs = {'trust_remote_code': True}\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=f\"{EMBEDDING_ROOT}{embeddings_choice}\\\\\", model_kwargs=model_kwargs)\n",
    "else:\n",
    "    if (embeddings_choice == \"openai\"):\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_KEY)\n",
    "\n",
    "# Set up ChromaDB based on whether or not pre-saved information should be used\n",
    "context_locs = [PDF_ROOT, CSV_ROOT]\n",
    "chroma_load = set_chroma_load(MODIFIED_ROOT, CHROMA_ROOT, embeddings_choice, other_locs=context_locs)\n",
    "if chroma_load:\n",
    "    db_chroma = Chroma(embedding_function=embeddings, persist_directory=cur_embed_db)\n",
    "else:\n",
    "    # Remove old folder\n",
    "    try:\n",
    "        rmtree(cur_embed_db)\n",
    "    except Exception as e:\n",
    "        print(f\"Error:\\n{e}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Create new folder\n",
    "    try:\n",
    "        os.mkdir(cur_embed_db)\n",
    "    except Exception as e:\n",
    "        print(f\"Error:\\n{e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Give context information to Chroma\n",
    "    # Not sure best way to handle, so create Chroma with first set of documents, then add any other documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = load_and_chunk(text_splitter, pdf_loc=PDF_ROOT, csv_loc=CSV_ROOT)\n",
    "    db_chroma = Chroma.from_documents(chunks['pdf'], embeddings, persist_directory=cur_embed_db)\n",
    "    for key in chunks.keys():\n",
    "        if chunks[key] is not None:\n",
    "            db_chroma.add_documents(documents=chunks[key])\n",
    "\n",
    "    # Save current time as last modified time for context information for this embedding\n",
    "    with open(f\"{MODIFIED_ROOT}{embeddings_choice}.txt\", \"w\") as outf:\n",
    "        outf.write(f\"{time()}\")\n",
    "\n",
    "    # Flip chroma_load to True, to allow rerunning this section without remaking Chroma database\n",
    "    chroma_load = True\n",
    "\n",
    "# Set up model based on model_choice\n",
    "if local:\n",
    "    if (model_choice in ollama_models):\n",
    "        model = ChatOllama(model=model_choice)\n",
    "    elif (model_choice in local_models):\n",
    "        llm = HuggingFacePipeline.from_model_id(model_id=f\"{MODEL_ROOT}{model_choice}\\\\\", task=\"text-generation\", device=0)\n",
    "        model = ChatHuggingFace(llm=llm)\n",
    "else:\n",
    "    if (model_choice == \"openai\"):\n",
    "        model = ChatOpenAI(openai_api_key=OPENAI_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326591d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Welcome to the experimental RAG system! Enter a prompt, or 'exit' to exit.\n",
      "\n",
      "How can I help?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The role of NR4A1 in chondrocyte apoptosis during osteoarthritis involves promoting the death of cartilage cells through specific signaling pathways.\n",
      "\n",
      "1. **Role Activation**: NR4A1 facilitates chondrocyte apoptosis, a process that leads to joint damage characteristic of osteoarthritis.\n",
      "   \n",
      "2. **Pathway Involvement**: It plays a key role in two primary pathways of apoptosis:\n",
      "   - **Mitochondrial Apoptosis Pathway**: This pathway is associated with the release of cytochrome c and involves BAX activation, contributing to DNA fragmentation and cell death.\n",
      "   - **p38 MAPK Pathway**: NR4A1 supports this pathway, which may be involved in regulating mitochondrial function and apoptosis.\n",
      "\n",
      "3. **Cellular Stressors**: The activation or modulation of these pathways can be influenced by various stressors within the cartilage cells, such as oxidative stress, affecting their ability to maintain structural integrity.\n",
      "\n",
      "In summary, NR4A1 is crucial for driving chondrocyte apoptosis during osteoarthritis through its involvement in mitochondrial and MAPK pathways.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Welcome to the experimental RAG system! Enter a prompt, or 'exit' to exit.\n",
      "\n",
      "How can I help?\n",
      "\n",
      "\n",
      "Thank you for using the RAG system!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JIM Laptop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Receive answer to a query, with ability to save to .txt file.\"\"\"\n",
    "\n",
    "while True:\n",
    "    # Receive response to query\n",
    "    print(\"\\n\\nWelcome to the experimental RAG system! Enter a prompt, or 'exit' to exit.\")\n",
    "    print(\"\\nHow can I help?\\n\")\n",
    "\n",
    "    query = None\n",
    "    try:\n",
    "        query = input()\n",
    "    except KeyboardInterrupt as e:\n",
    "        if query is not None:\n",
    "            pass\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    elif query == \"\":\n",
    "        print(\"Please enter a query.\")\n",
    "        continue\n",
    "\n",
    "    response = answer_query(query, db_chroma, 5)\n",
    "\n",
    "    # Create output for question and response\n",
    "    output = \"\\n\"\n",
    "\n",
    "    # Extract string of response, if needed\n",
    "    if not isinstance(response, str):\n",
    "        response = response.content\n",
    "\n",
    "    # Add response to output\n",
    "    if local:\n",
    "        if (model_choice == \"Mistral-7B-Instruct-v0.3\"):\n",
    "            prompt_end = response.find(\"[/INST]\")\n",
    "            output += response[(prompt_end + 7):]\n",
    "        elif (\"deepseek-r1\" in model_choice):\n",
    "            think_end = response.find(\"</think>\")\n",
    "            output += response[(think_end + 8):]\n",
    "        else:\n",
    "            output += response\n",
    "    else:\n",
    "        output += response\n",
    "\n",
    "    # Print response for convenience\n",
    "    print(output)\n",
    "\n",
    "print(\"\\nThank you for using the RAG system!\")\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b168d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
